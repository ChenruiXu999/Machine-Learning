{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9vxzLzaNzc_"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable #带有自动求导的变量封装\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    '''d_model是词嵌入的维度，vocab是词表的大小'''\n",
        "    def __init__(self,d_model,vocab):\n",
        "        super(Embeddings,self).__init__()\n",
        "        self.lut=nn.Embedding(vocab,d_model)\n",
        "        self.d_model=d_model\n",
        "    def forward(self,x):\n",
        "        return self.lut(x)*math.sqrt(self.d_model)#根号维度的作用：缩放"
      ],
      "metadata": {
        "id": "1Cv5YO5dQbEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,d_model,dropout,max_len=5000):\n",
        "        super(PositionalEncoding,self).__init__()\n",
        "        self.dropout=nn.Dropout(p=dropout)\n",
        "        #初始化一个位置编码，内容是0，矩阵的大小是max_len * d_model [5000,512]\n",
        "        pe=torch.zeros(max_len,d_model)\n",
        "        #初始化一个绝对位置矩阵,绝对位置其实就是用它的索引去表示，unsqueeze成一列 [5000,1]\n",
        "        position=torch.arange(0,max_len).unsqueeze(1)\n",
        "        #之后考虑如何将这些位置信息加入到位置编码矩阵中\n",
        "        #即,我们需要把max_len*1 转化成 max_len*d_model的形式，这样的话，我们需要给max_len*1的矩阵多乘以一个1*d_model的把一维扩张成d_model维的矩阵\n",
        "        #同时还希望它能够将自然数的绝对位置编码缩放成足够小的数字，有助于之后梯度下降时的快速收敛\n",
        "        #定义一个变化矩阵div_term，跳跃式的变化,初始化两个，用于分别sin/cos跳跃式的计数\n",
        "        div_term=torch.exp(torch.arange(0,d_model,2)*-(math.log(10000)/d_model)) #[256]的list\n",
        "        pe[:,0::2]=torch.sin(position*div_term)#[5000,256]\n",
        "        pe[:,1::2]=torch.cos(position*div_term)#[5000,256]. 总共是[5000,512]\n",
        "        #pe现在还只是是一个二维矩阵，需要unsqueeze拓展一个维度\n",
        "        pe=pe.unsqueeze(0) #[1,5000,512]\n",
        "        #最后一步需要把pe的位置编码注册成模型的buffer\n",
        "        #什么是buffer：不需要根据training进行更新，没有参数和超参数\n",
        "        self.register_buffer('pe',pe)\n",
        "    def forward(self,x):\n",
        "        x=x+Variable(self.pe[:,:x.size(1)],requires_grad=False)#只提取x对应长度既可，不需要5000\n",
        "        return self.dropout(x)\n",
        "        #这里为什么要对带有位置编码的x dropout\n",
        "        #这里的dropout可以看成是对于数据进行一种添加噪声的方法，增加模型的鲁棒性。"
      ],
      "metadata": {
        "id": "fBbW0zIsbwpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def subsequent_mask(size):\n",
        "    #生成掩码张量\n",
        "    attn_shape=(1,size,size)\n",
        "    #用torch.one的方法生成值为1的上三角矩阵，为什么不是全矩阵：节省空间\n",
        "    #剩下没有被定义的元素位置，给予无符号8位整形 uint8\n",
        "    #为什么做成上三角：每次遍历一行，每次依次往后增加一个mask\n",
        "    subsequent_mask=np.triu(np.ones(attn_shape),k=1).astype('uint8')\n",
        "    #最后把torch转化为tensor，内部做一个1- 的操作，实际是在做一个矩阵反转，现在是一个元素为1的下三角矩阵\n",
        "    return torch.from_numpy(1-subequent_mask)"
      ],
      "metadata": {
        "id": "23jeLYEEbwrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#attention(Q,K,V)=softmax((Q*K)/(sqrt(dk)))*V\n",
        "#query就是整个文本的信息\n",
        "#key:为了让模型更好的理解文本，给出的关键提示就是key\n",
        "#value就是对应的权重，对应query的权重，权重高的就是这段文本所对应的答案\n",
        "#初始化的时候，value不知道哪些是重点，所以v和k的矩阵一样\n",
        "#自注意力机制：一种特殊情况，QKV三个矩阵一样\n",
        "def attention(query,key,value,mask=None,dropout=None):\n",
        "    d_k=query.size(-1)#首先取query最后一维的大小，当作词嵌入的维度,之后用于缩放\n",
        "    scores=torch.matmul(query,key.transpose(-2,-1))/math.sqrt(d_k)#key中的二维矩阵要转置\n",
        "    if mask!=None:#如果有掩码张量\n",
        "        scores=scores.mask_fill(mask==0,-1e9)\n",
        "    p_attn=F.softmax(scores,dim=-1)\n",
        "    #判断有没有dropout\n",
        "    if dropout is not None:\n",
        "        p_attn=dropout(p_attn)\n",
        "    return torch.matmul(p_attn,value),p_attn"
      ],
      "metadata": {
        "id": "hwDpyBICbwth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy#用于深度拷贝\n",
        "def clones(module,n):#n代表要克隆的数量\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(n)])\n",
        "\n",
        "#多头的意义：一个词被映射成512个维度，这么多维度不可能用一个head处理，所以应该用多个头来处理这些维度\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self,head,embedding_dim,dropout=0.1):#embedding_dim是词嵌入的维度,512。不是有多少个词\n",
        "        super(MultiHeadedAttention,self).__init__()\n",
        "        assert embedding_dim%head==0\n",
        "        self.d_k=embedding_dim//head#获得每个头获得的分割词向量维度d_k\n",
        "        self.head=head\n",
        "        self.embedding_dim=embedding_dim\n",
        "        #feature_in是embedding_dim, out是embedding\n",
        "        self.linears=clones(nn.Linear(embedding_dim,embedding_dim),4)#QKV分别需要一个linear，最后拼接也需要一个\n",
        "        self.attn=None\n",
        "        self.dropout=nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self,query,key,value,mask=None):\n",
        "        if mask is not None:\n",
        "            mask=mask.unsqueeze(1)#增加一个维度，方便形成batch\n",
        "        batch_size=query.size(0)\n",
        "        #用zip把QKV三个矩阵组合到一起\n",
        "        query,key,value=\\\n",
        "        [model(x).view(batch_size,-1,self.head,self.d_k).transpose(1,2)#转置2，3这两个维度\n",
        "        for model,x in zip(self.linears,(query,key,value))]#对应的model加载对应的data\n",
        "        #得到每个头的输入之后我们就可以把他们传入到attention里了\n",
        "        x,self.attn=attention(query,key,value,mask=None,dropout=self.dropout)\n",
        "        x=x.transpose(1,2).contiguous().view(batch_size,-1,self.head*self.d_k)#如果想同时用transpose和view，需要用contiguous\n",
        "        return self.linears[-1](x)"
      ],
      "metadata": {
        "id": "qShEPC1Ubwv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前馈全连接层\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self,d_model,d_ff,dropout=0.1):#d_model是第一个线性层输入的维度\n",
        "        super(PositionwiseFeedForward,self).__init__()#d_ff隐藏\n",
        "        self.w1=nn.Linear(d_model,d_ff)\n",
        "        self.w2=nn.Linear(d_ff,d_model)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        return self.w2(self.dropout(F.relu(self.w1(x))))#实现两层的全连接"
      ],
      "metadata": {
        "id": "7pFL-Qx3Lt3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#规范化层\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self,features,eps=1e-6):\n",
        "        super(LayerNorm,self).__init__()\n",
        "        self.a2=nn.Parameter(torch.ones(features))#定义两个辅助张量\n",
        "        self.b2=nn.Parameter(torch.zeros(features))#Parameter进行封装\n",
        "        self.eps=eps\n",
        "    def forward(self,x):\n",
        "        mean=x.mean(-1,keepdim=True)#求最后一个维度的mean\n",
        "        std=x.std(-1,keepdim=True)\n",
        "        return self.a2*(x-mean)/(std+self.eps)+self.b2#b2这里就是shift，*这里是对应位置相乘"
      ],
      "metadata": {
        "id": "saxVZF14Lt5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add层\n",
        "class SublayerConnection(nn.Module):\n",
        "    def __init__(self,size,dropout=0.1):#size是词嵌入维度大小\n",
        "        super(SublayerConnection,self).__init__()\n",
        "        self.norm=LayerNorm(size)\n",
        "        self.dropout=nn.Dropout(p=dropout)\n",
        "    def forward(self,x,sublayer):\n",
        "        return x+self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "Ei7bdoKCTLBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#汇总：编码器层\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self,size,self_attn,feed_forward,dropout):#这里feed_forward是一个维度\n",
        "        super(EncoderLayer,self).__init__()\n",
        "        self.self_attn=self_attn\n",
        "        self.feed_forward=feed_forward\n",
        "        self.sublayer=clones(SublayerConnection(size,dropout),2)#我们需要两个子连接层\n",
        "        self.size=size\n",
        "\n",
        "    def forward(self,x,mask):\n",
        "        x=self.sublayer[0](x,lambda x:self.self_attn(x,x,x,mask))\n",
        "        return self.sublayer[1](x,self.feed_forward)"
      ],
      "metadata": {
        "id": "22INwyOphYCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#编码器\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,layer,N):#初始化编码器层和层数\n",
        "        super(Encoder,self).__init__()\n",
        "        self.layers=clones(layer,N)#先定义好n个layer\n",
        "        self.norm=LayerNorm(layer.size)#512\n",
        "        \n",
        "    def forward(self,x,mask):\n",
        "        for layer in self.layers:\n",
        "            x=layer(x,mask)#每层的输入是x和mask，然后迭代输出，定义每次新的输入为x\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "s6AFkK_0sRBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#流程模拟\n",
        "d_model=512\n",
        "dropout=0.1\n",
        "max_len=60\n",
        "vocab=1000\n",
        "#词嵌入演示\n",
        "embedding=Embeddings(512,1000)#这里的10参考的是下面一行数值的取值范围\n",
        "x=Variable(torch.LongTensor([[100,2,421,508],[491,998,1,221]]))\n",
        "print('input shape:',x.shape,' || And embedding dimension:', d_model)\n",
        "embr=embedding(x)\n",
        "print(embr.shape)\n",
        "#print(embr)"
      ],
      "metadata": {
        "id": "-BiFA8zImedj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#位置编码演示\n",
        "x=embr\n",
        "pe=PositionalEncoding(d_model=512,dropout=0.1,max_len=60)\n",
        "pe_result=pe(embr)\n",
        "print(pe_result.shape)\n",
        "#print(pe_result)"
      ],
      "metadata": {
        "id": "w3isWawmnq3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mask\n",
        "x=Variable(torch.randn(5,5))\n",
        "print(x)\n",
        "mask=Variable(torch.zeros(5,5))\n",
        "print(mask)\n",
        "y=x.masked_fill(mask==0,-1e9)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "WS9j9Im9qcuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#attention\n",
        "query=key=value=pe_result\n",
        "attn,p_attn=attention(query,key,value)\n",
        "#print('attn:',attn)\n",
        "print('attn.shape:',attn.shape)\n",
        "#print('p_attn:',p_attn)"
      ],
      "metadata": {
        "id": "QUD6321EPRW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#多头\n",
        "head=8\n",
        "embedding_dim=512\n",
        "dropout=0.2\n",
        "mask=Variable(torch.zeros(8,4,4))\n",
        "multi_head=MultiHeadedAttention(head=8,embedding_dim=512,dropout=0.2)\n",
        "multi_head_result=multi_head(query,key,value,mask)\n",
        "#print(multi_head_result)\n",
        "print(multi_head_result.shape)"
      ],
      "metadata": {
        "id": "9CecuPU4eKN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#全连接层\n",
        "d_model=512\n",
        "d_ff=64\n",
        "dropout=0.2\n",
        "x=multi_head_result\n",
        "fc=PositionwiseFeedForward(d_model=512,d_ff=64,dropout=0.2)\n",
        "fc_result=fc(x)\n",
        "#print(fc_result)\n",
        "print(fc_result.shape)"
      ],
      "metadata": {
        "id": "iMCSrtlXqNtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#正则化\n",
        "feature=d_model=512\n",
        "eps=1e-6\n",
        "x=fc_result\n",
        "layer_norm=LayerNorm(features=512,eps=1e-6)\n",
        "layer_norm_result=layer_norm(x)\n",
        "#print(layer_norm_result)\n",
        "print(layer_norm_result.shape)"
      ],
      "metadata": {
        "id": "r7RHIbHrr4Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resnet\n",
        "d_model=512\n",
        "x=pe_result\n",
        "mask=Variable(torch.zeros(8,4,4))#因为head是8个所以这里mask也需要8个\n",
        "self_attn=MultiHeadedAttention(head=8,embedding_dim=512,dropout=0.2)#head=8,embedding_dim=512,dropout=0.2\n",
        "sublayer=lambda x: self_attn(x,x,x,mask)\n",
        "sc=SublayerConnection(size=512,dropout=0.2)\n",
        "sc_result=sc(x,sublayer)\n",
        "#print(sc_result)\n",
        "print(sc_result.shape)"
      ],
      "metadata": {
        "id": "9FVZQjzxtwjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoder_Layer\n",
        "d_ff=64\n",
        "d_model=512\n",
        "size=512\n",
        "ff=PositionwiseFeedForward(d_model,d_ff,dropout=0.2) \n",
        "self_attn=MultiHeadedAttention(head=8,embedding_dim=512,dropout=0.2)#head=8,embedding_dim=512,dropout=0.2\n",
        "encoder=EncoderLayer(size,self_attn,ff,dropout)#size,self_attn,feed_forward,dropout\n",
        "encoder_result=encoder(x,mask)\n",
        "#print(encoder_result)\n",
        "print(encoder_result.shape)"
      ],
      "metadata": {
        "id": "QtYHKTEw1H7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoder\n",
        "c=copy.deepcopy\n",
        "attn=MultiHeadedAttention(head=8,embedding_dim=512,dropout=0.2)\n",
        "ff=PositionwiseFeedForward(d_model,d_ff,dropout)\n",
        "layer=EncoderLayer(size,c(attn),c(ff),dropout)\n",
        "N=8\n",
        "ENCODER=Encoder(layer,N)\n",
        "EN_result=ENCODER(x,mask)\n",
        "#print(EN_result)\n",
        "print(EN_result.shape)"
      ],
      "metadata": {
        "id": "_hpOvBvBsREI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#解码器层\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self,size,self_attn,src_attn,feed_forward,dropout):\n",
        "        #size是词嵌入维度大小，同时也代表解码器层的尺寸；self_attn: Q=K=V；src_attn: Q!=K=V\n",
        "        super(DecoderLayer,self).__init__()\n",
        "        self.size=size\n",
        "        self.self_attn=self_attn\n",
        "        self.src_attn=src_attn\n",
        "        self.feed_forward=feed_forward\n",
        "        self.sublayer=clones(SublayerConnection(size,dropout),3)#三层对应output的多头，加上encoder的多头和全连接层\n",
        "\n",
        "    def forward(self,x,memory,source_mask,target_mask):\n",
        "        #memory是来自编码层的语义存储的变量，以及元数据掩码张量和目标数据的掩码张量\n",
        "        m=memory\n",
        "        #第一层，这里的x不是encoder里面的x（encoder里面的x是现在的memory），这里的x是对应的是数据集中的结果y\n",
        "        #这里的mask才开始有用，之前encoder的mask是全零矩阵\n",
        "\n",
        "        x=self.sublayer[0](x,lambda x: self.self_attn(x,x,x,target_mask))\n",
        "        #第二层，src_attn，这里的Q我们取x，K，V；source_mask\n",
        "        x=self.sublayer[1](x,lambda x: self.src_attn(x,m,m,source_mask))\n",
        "        #第三层\n",
        "        return self.sublayer[2](x,self.feed_forward)"
      ],
      "metadata": {
        "id": "W7h2Z0Ku0Vur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#解码器\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,layer,N):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.layers=clones(layer,N)\n",
        "        self.norm=LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self,x,memory,source_mask,target_mask):\n",
        "        for layer in self.layers:\n",
        "            x=layer(x,memory,source_mask,target_mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "2s_vMCq6JBb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#输出部分\n",
        "import torch.nn.functional as F\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self,d_model,voacb_size):\n",
        "        super(Generator,self).__init__()\n",
        "        self.project=nn.Linear(d_model,voacb_size)\n",
        "\n",
        "    def forward(self,x):\n",
        "        return F.log_softmax(self.project(x),dim=-1)"
      ],
      "metadata": {
        "id": "SYOlFTmHP4uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#终极整体\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self,encoder,decoder,source_embed,target_embed,generator):\n",
        "        super(EncoderDecoder,self).__init__()\n",
        "        self.encoder=encoder#编码对象\n",
        "        self.decoder=decoder#解码对象\n",
        "        self.src_embed=source_embed#原数据的embedding函数\n",
        "        self.tgt_embed=target_embed#目标数据embedding函数\n",
        "        self.generator=generator\n",
        "    def forward(self,source,target,source_mask,target_mask):\n",
        "        #source是原数据,target是目标数据\n",
        "        return self.decode(self.encode(source,source_mask),source_mask,target,target_mask)\n",
        "    def encode(self,source,source_mask):\n",
        "        return self.encoder(self.src_embed(source),source_mask)#src_embed就是词嵌入\n",
        "    def decode(self,memory,source_mask,target,target_mask):\n",
        "        return self.decoder(self.tgt_embed(target),memory,source_mask,target_mask)\n"
      ],
      "metadata": {
        "id": "xZ3xx3s6U_pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#decoder_layer\n",
        "size=d_model=512\n",
        "head=8\n",
        "d_ff=64\n",
        "dropout=0.2\n",
        "x=pe_result\n",
        "self_attn=src_attn=MultiHeadedAttention(head,d_model,dropout)\n",
        "ff=PositionwiseFeedForward(d_model,d_ff,dropout)\n",
        "memory=EN_result\n",
        "mask=Variable(torch.zeros(8,4,4))\n",
        "source_mask=target_mask=mask\n",
        "\n",
        "decode_layer=DecoderLayer(size,self_attn,src_attn,ff,dropout)\n",
        "decode_layer_result=decode_layer(x,memory,source_mask,target_mask)\n",
        "print(decode_layer_result.shape)"
      ],
      "metadata": {
        "id": "SMClzpejEg_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#decoder\n",
        "layer=DecoderLayer(d_model,c(attn),c(attn),c(ff),dropout)\n",
        "decoder=Decoder(layer,8)\n",
        "decoder_result=decoder(x,memory,source_mask,target_mask)\n",
        "print(decoder_result)\n",
        "print(decoder_result.shape)"
      ],
      "metadata": {
        "id": "GUC2pSzqK4Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final\n",
        "d_model=512\n",
        "vocab_size=1000\n",
        "x=decode_layer_result\n",
        "generator=Generator(d_model,vocab_size)\n",
        "generator_result=generator(x)\n",
        "print(generator_result)\n",
        "print(generator_result.shape)"
      ],
      "metadata": {
        "id": "vcyjA-OWSZn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#enco-deco\n",
        "vocab_size=1000\n",
        "d_model=512\n",
        "encoder=ENCODER\n",
        "decoder=decoder\n",
        "source_embed=nn.Embedding(vocab_size,d_model)\n",
        "target_embed=nn.Embedding(vocab_size,d_model)\n",
        "\n",
        "source=target=Variable(torch.LongTensor([[100,2,421,508],[491,998,1,221]]))\n",
        "source_mask=target_mask=Variable(torch.zeros(8,4,4))\n",
        "gen=generator\n",
        "ED=EncoderDecoder(encoder,decoder,source_embed,target_embed,gen)\n",
        "ed_result=ED(source,target,source_mask,target_mask)\n",
        "print(ed_result)\n",
        "print(ed_result.shape)"
      ],
      "metadata": {
        "id": "QMDlg2dNTDxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(source_vocab,target_vocab,N=6,d_model=512,d_ff=2048,head=8,dropout=0.1):\n",
        "    c=copy.deepcopy\n",
        "    attn=MultiHeadedAttention(head,d_model)\n",
        "    ff=PositionwiseFeedForward(d_model,d_ff,dropout)\n",
        "    position=PositionalEncoding(d_model,dropout)\n",
        "    model=EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model,c(attn),c(ff),dropout),N),\n",
        "        Decoder(DecoderLayer(d_model,c(attn),c(attn),c(ff),dropout),N),\n",
        "        nn.Sequential(Embeddings(d_model,source_vocab),c(position)),\n",
        "        nn.Sequential(Embeddings(d_model,target_vocab),c(position)),\n",
        "        Generator(d_model,target_vocab)\n",
        "        )\n",
        "    for p in model.parameters():#初始化模型的参数，初始化城一个均匀分布的矩阵\n",
        "        if p.dim()>1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model\n",
        "\n",
        "if __name__ =='__main__':\n",
        "    res=make_model(source_vocab=11,target_vocab=11,N=6)\n",
        "    print(res)"
      ],
      "metadata": {
        "id": "8-xgYdiGQIsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def data_generator(V,batch,num_batch):\n",
        "    # 该函数用于随机生成copy函数的数据，它的三个输入参数是 V随机生成数字的最大值+1\n",
        "    #batch:每次输送给模型更新一次参数的数据量，Num_batch:一共输送模型多少轮数据\n",
        "\n",
        "    #使用for循环变量n batches\n",
        "    for i in range(num_batch):\n",
        "        #在循环中使用np的random.randint方法随机生成【1，v]的整数\n",
        "        #分布在（batch,10)形状的矩阵中，然后再把numpy形式转换成torch的tensor\n",
        "        data =torch.from_numpy(np.random.randint(1,V,size=(batch,10)))\n",
        "\n",
        "        #接着使矩阵的第一列数字都为1，这一列也就成为了其实标志列\n",
        "        #当解码器进行第一次解码时，会使用起始标志列作为输入\n",
        "        data[:,0]=1\n",
        "\n",
        "        #因为是copy任务，所有source与target是完全相同的，且数据样本作用变量不需要梯度\n",
        "        # 因此requires_grad设置为false\n",
        "        source=Variable(data,requires_grad=False)\n",
        "        target=Variable(data,requires_grad=False)\n",
        "\n",
        "        # 使用batch对source和target进行对应批次的掩码张量生成，最后使用yield返回\n",
        "        yield Batch(source,target)"
      ],
      "metadata": {
        "id": "gJ2KJ0RN8td2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#定义模型优化器和损失函数 \n",
        "!pip install pyitcast\n",
        "#导入模型单轮训练工具包run_epoch,该工具将对模型使用给定的损失函数计算方法进行单轮参数更新\n",
        "from pyitcast.transformer_utils import run_epoch\n",
        "#导入工具包batch,它能够对原始样本数据生成对应批次的掩码张量\n",
        "from pyitcast.transformer_utils import Batch\n",
        "#导入工具包 get_std_opt,该工具获得标准优化器，使其对序列到序列的任务更有效\n",
        "from pyitcast.transformer_utils import get_std_opt\n",
        "#导入标签平滑工具包，小幅度的改变原有标签的值域，可以防止过拟合(人工标注的数据也可能因为外界影响而产生偏差)\n",
        "from pyitcast.transformer_utils import LabelSmoothing\n",
        "#导入计算损失包，该工具能够使用标签平滑后的结果进行损失的计算，损失的计算方法是交叉熵损失函数\n",
        "from pyitcast.transformer_utils import SimpleLossCompute\n",
        "\n",
        "V=11\n",
        "batch_size=20\n",
        "num_batch=30\n",
        "model=make_model(V,V,N=2)\n",
        "model_optimizer=get_std_opt(model)#优化器\n",
        "criterion=LabelSmoothing(size=V,padding_idx=0,smoothing=0.0)\n",
        "loss=SimpleLossCompute(model.generator,criterion,model_optimizer)#loss"
      ],
      "metadata": {
        "id": "XVEHGS4zeqy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyitcast.transformer_utils import greedy_decode#对最终结果进行贪婪解码,每次预测都选择最大概率进行输出\n",
        "def run(model,loss,epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        run_epoch(data_generator(V,8,20),model,loss)\n",
        "        model.eval()\n",
        "        run_epoch(data_generator(V,8,5),model,loss)\n",
        "    model.eval()\n",
        "    source=Variable(torch.LongTensor([[1,3,2,5,4,6,7,8,9,10]]))\n",
        "    source_mask=Variable(torch.ones(1,1,10))\n",
        "    result=greedy_decode(model,source,source_mask,max_len=10,start_symbol=1)\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "c5k-zMWZflYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ =='__main__':\n",
        "    run(model,loss)"
      ],
      "metadata": {
        "id": "13BlZfHhvy_9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}